# Non-linear Activations (weighted sum, nonlinearity)

1. Tensor  
2. Dataset,DataLoader  
   1. `torch.utils.data`
3. Transform
   1. `torchvision.transforms.v2`
      1. `v2.CutMix`
      2. `v2.MixUp`
   2. `torchvision.transforms`
4. nn-model  
   1. `torch.nn`
        1. Convolution Layers
            1. Convolution
               1. `nn.Conv#d`
               2. `nn.ConvTranspose#d`
        2. Pooling Layers
        3. **Non-Linear Activations**
5. Automatic-Differentiation  
6. Parameter-Optimization  
7. model-save-load  
---

## nn.Non-Linear Activations (weighted sum, nonlinearity)
- `nn.ELU` : Applies the Exponential Linear Unit (ELU) function, element-wise. *
- `nn.Hardshrink` : Applies the Hard Shrinkage (Hardshrink) function element-wise. *
- `nn.Hardsigmoid` : Applies the Hardsigmoid function element-wise. *
- `nn.Hardtanh` : Applies the HardTanh function element-wise. *
- `nn.Hardswish` : Applies the Hardswish function, element-wise. *
- `nn.LeakyReLU` : Applies the LeakyReLU function element-wise.
- `nn.LogSigmoid` : Applies the Logsigmoid function element-wise.
- `nn.MultiheadAttention` : Allows the model to jointly attend to information from different representation subspaces.
- `nn.PReLU` : Applies the element-wise PReLU function.
- `nn.ReLU` : Applies the rectified linear unit function element-wise.
- `nn.ReLU6` : Applies the ReLU6 function element-wise.
- `nn.RReLU` : Applies the randomized leaky rectified linear unit function, element-wise.
- `nn.SELU` : Applies the SELU function element-wise.
- `nn.CELU` : Applies the CELU function element-wise.
- `nn.GELU` : Applies the Gaussian Error Linear Units function.
- `nn.Sigmoid` : Applies the Sigmoid function element-wise.
- `nn.SiLU` : Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
- `nn.Mish` : Applies the Mish function, element-wise.
- `nn.Softplus` : Applies the Softplus function element-wise.
- `nn.Softshrink` : Applies the soft shrinkage function element-wise.
- `nn.Softsign` : Applies the element-wise Softsign function.
- `nn.Tanh` : Applies the Hyperbolic Tangent (Tanh) function element-wise. *
- `nn.Tanhshrink` : Applies the element-wise Tanhshrink function. *
- `nn.Threshold` : Thresholds each element of the input Tensor.
- `nn.GLU` : Applies the gated linear unit function.

## Non-linear Activations (other)

- `nn.Softmin` : Applies the Softmin function to an n-dimensional input Tensor.
- `nn.Softmax` : Applies the Softmax function to an n-dimensional input Tensor.
- `nn.Softmax2d` : Applies SoftMax over features to each spatial location.
- `nn.LogSoftmax` : Applies the log(Softmax(ğ‘¥)) function to an n-dimensional input Tensor.
- `nn.AdaptiveLogSoftmaxWithLoss` : Efficient softmax approximation.

[Survey of Activation Functions](https://neverabandon.tistory.com/8)

[wikidocs.net](https://wikidocs.net/60683)

[ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì˜ ì˜ë¯¸ì™€ í•´ê²°ë°©ë²•](https://heytech.tistory.com/388)

[Gradient of ReLu at 0](https://discuss.pytorch.org/t/gradient-of-relu-at-0/64345/4)  

[Gradients for non-differentiable functions](https://pytorch.org/docs/stable/notes/autograd.html#gradients-for-non-differentiable-functions)

[Natural Gradientë¥¼ ìœ„í•´ ë³´ë©´ ì¢‹ì„ ê¸€](https://rlwithme.tistory.com/5)

## ì™œ Non-linear Activations ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

Linear Activationì˜ ê²½ìš° ë‹¨ìˆœí•œ í–‰ë ¬ ê³±ì¸ë°, ì´ëŠ” ë‹¨ìˆœíˆ ê´„í˜¸ë¥¼ í’€ì–´ ê³„ì‚°í•  ê²½ìš°ì— ë‹¨ í•œ ê°œì˜ Activation functionìœ¼ë¡œ ëŒ€ì²´ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì´ëŠ” ë§ì€ ì¸µì„ ì¶”ê°€í•´ê°€ë©´ì„œ ê° ì¸µë³„ë¡œ í™œì„±í•¨ìˆ˜ê°€ ì ìš©ë˜ì–´ nodeë“¤ ê°„ì˜ ì´˜ì´˜í•œ ê´€ê³„ë¥¼ êµ¬í˜„í•˜ê² ë‹¤ëŠ” ì‹ ê²½ë§ êµ¬í˜„ ë°©ì‹ê³¼ ë§ì§€ ì•Šë‹¤.

> ë‹¤ë§Œ, ì„ í˜• í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì¸µì„ 'ì€ë‹‰ì¸µ'(ë¹„ì„ í˜• í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”) ê³¼ êµ¬ë³„í•´ 'ì„ í˜•ì¸µ'ì´ë¼ëŠ” ë³„ê°œì˜ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê¸°ë„ í•  ì •ë„ë¡œ ê·¸ ìì²´ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” weightì´ ìƒê²¨ë‚œë‹¤ëŠ” ì¸¡ë©´ì—ì„œ ì˜ë¯¸ê°€ ì•„ì£¼ ì—†ì§€ëŠ” ì•Šë‹¤. [wikidocs.net](https://wikidocs.net/60683)

## Vanishing Gradient

[ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì˜ ì˜ë¯¸ì™€ í•´ê²°ë°©ë²•](https://heytech.tistory.com/388)

ê¸°ìš¸ê¸° ì†Œì‹¤ì´ë€ ì—­ì „íŒŒë¥¼ ì´ìš©í•œ weight ì¡°ì • ê³¼ì •ì—ì„œ Chain Ruleì— ë”°ë¼ ê³„ì†í•´ì„œ gradientë¥¼ ê³±í•˜ê²Œ ë˜ëŠ”ë°, ì´ gradient ê°’ì´ ê³„ì†í•´ì„œ ì‘ì•„ì ¸ ì…ë ¥ì¸µì— ê°€ê¹Œì›Œì¡Œì„ ë•Œ ì¯¤ì—” 0ì— ê·¼ì‚¬í•´ì§€ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

### Sigmoid í•¨ìˆ˜

$$
S(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}
$$

<p align="center">
<img src="./assets/0702sigmoid.png" style="width:35%" />
</p>


$$
S'(x) = S(x)(1-S(x))
$$

ë¯¸ë¶„ ê³„ì‚°ì˜ í¸ì˜ì„±ê³¼ ë‰´ëŸ°ì˜ ì‹ ê²½ì‹ í˜¸ ì „ë‹¬ ë°©ì‹ì„ ë§¤ìš° ê°€ê¹ê²Œ êµ¬í˜„í•œë‹¤ëŠ” ì ì—ì„œ ë„ë¦¬ ì‚¬ìš©ëë˜ Sigmoid í•¨ìˆ˜.
ê·¸ ë¯¸ë¶„ì€ $S(x) = 0.5$ ì—ì„œ ìµœëŒ“ê°’ 0.25ë¥¼ ê°€ì§„ë‹¤. ë”°ë¼ì„œ, ëª¨ë“  $x$ì˜ ë²”ìœ„ ë‚´ì—ì„œ ìµœëŒ€ 0.25 , ìµœì†Œ 0ì˜ ê°’ì„ ê°€ì§€ë¯€ë¡œ, ì´ ë¯¸ë¶„ê°’(gradient)ë¥¼ ê³„ì†í•´ì„œ ê³±í•˜ë‹¤ ë³´ë©´, ê·¸ gradientëŠ” ê·¹íˆ ì‘ì•„ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤.

> ì´ì— ë”í•´, exp(x)ì˜ ê³„ì‚°ì„ ê·¼ì‚¬ì¹˜ë¡œ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì»´í“¨í„°ì˜ ê³„ì‚°ì˜¤ì°¨ë„ ì—­ì‹œ ê³„ì†í•´ì„œ ê°€ì‚°ëœë‹¤.

#### Bias Shift(í¸í–¥ ì´ë™)
Sigmoid í•¨ìˆ˜ì˜ Vanishing gradientë¥¼ ê°€ì†ì‹œí‚¤ëŠ” ì´ìœ ê°€ í•˜ë‚˜ ë” ìˆëŠ”ë°, ë°”ë¡œ **Bias Shift** (í¸í–¥ ì´ë™) ì´ë‹¤.
Sigmoid í•¨ìˆ˜ì˜ í‰ê· ì€ 0ì´ ì•„ë‹Œ 0.25ì¸ë°, ì´ëŠ” ì¦‰ ì…ë ¥ê°’ì˜ í•©ë³´ë‹¤ ì¶œë ¥ì¸µì˜ í•©ì´ ë” ì»¤ì§ˆ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì´ê²ƒì€ ë‹¤ì‹œ, gradientì˜ ë¶„í¬ê°€ gradientì˜ ìµœëŒ“ê°’ì¸ 0.25ë¥¼ ê°€ì§€ëŠ” ì¤‘ê°„ë¶€ë³´ë‹¤ëŠ” ì–‘ ê·¹ë‹¨ì—ì„œ ê´€ì°°ë  í™•ë¥ ì´ ì ì  ì»¤ì§„ë‹¤ëŠ” ëœ»ì´ë‹¤.

### $\tanh$ í•¨ìˆ˜

$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

<p align="center">
<img src="./assets/0702tanh.png" style="width:35%" />
</p>

$$
f'(x) = 1-f(x)^2
$$

ìŒê³¡ íƒ„ì  íŠ¸ í•¨ìˆ˜ëŠ” ê·¸ í˜•íƒœê°€ sigmoidì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, ìµœëŒ“ê°’ 1ì„ ê°€ì ¸ sigmoidë³´ë‹¤ í›¨ì”¬ ë„‰ë„‰í•œ gradientë²”ìœ„ë¥¼ ê°€ì¡Œë‹¤. ê·¸ëŸ¬ë‚˜, ì—¬ì „íˆ ê³±í•  ìˆ˜ë¡ ì‘ì•„ì§€ëŠ” íŠ¹ì„±ì„ ë²—ì–´ë‚˜ì§€ëŠ” ëª»í–ˆë‹¤. ë‹¤ë§Œ, Bias Shift í˜„ìƒì€ í‰ê· ì´ 0ì´ê¸° ë•Œë¬¸ì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

> ìì—°ì–´ ì²˜ë¦¬, ìŒì„±ì¸ì‹ ì„ ìœ„í•œ **recurrent neural networksì—ì„œ ëŒ€ë¶€ë¶„ ì‚¬ìš©ë¨**

### ReLU (Rectified Linear Unit) í•¨ìˆ˜

$$
f(x) = max(0, x)
$$
<p align="center">
<img src="./assets/0702ReLU.png" style="width:35%" />
</p>

0ë³´ë‹¤ í° ë²”ìœ„ì—ì„œ ê¸°ìš¸ê¸° 1, ì‘ì€ ë²”ìœ„ì—ì„œ 0ì˜ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ë‹¤. 

> 0ì—ì„œëŠ” 0ì„ ëŒ€ì…í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ë“¯ í•˜ë‹¤.   
[Gradient of ReLu at 0](https://discuss.pytorch.org/t/gradient-of-relu-at-0/64345/4)  
[Gradients for non-differentiable functions](https://pytorch.org/docs/stable/notes/autograd.html#gradients-for-non-differentiable-functions)

ë”°ë¼ì„œ, vanishing gradient ë¬¸ì œëŠ” í•´ê²°ë˜ì—ˆì§€ë§Œ, í™œì„±í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ì§€ ëª»í•œ ë…¸ë“œëŠ” 0ì´ ê³±í•´ì ¸ ë‹¤ì‹œëŠ” í™œì„±í™” ë˜ì§€ ëª»í•œë‹¤ëŠ” (Dying ReLU) ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

### Leaky ReLU í•¨ìˆ˜

$$
LeakyReLU(x) = max(0,x) + negative\_slope * min(0,x)
$$

<p align="center">
<img src="./assets/0702LeakyReLu.png" style="width:35%" />
</p>

ReLU í•¨ìˆ˜ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ìŒì˜ ì…ë ¥ì´ ë“¤ì–´ì™”ì„ ë–„, 0ì´ ì•„ë‹Œ ë§¤ìš° ì‘ì€ ê¸°ìš¸ê¸°ë¥¼ ê³±í•œë‹¤. ì´ë¡œì¨, ë‰´ëŸ°ì´ ì£½ëŠ” ê²ƒì„ ì˜ˆë°©í•œë‹¤.

---

[Survey of Activation Functions](https://neverabandon.tistory.com/8) : ê°ì¢… í™œì„±í•¨ìˆ˜ë“¤ì— ëŒ€í•´ ì˜ ì •ë¦¬í•´ ë†“ì€ ê¸€


<p align="center">
<img src="./assets/0702FamilyOfActivationFunctions.png" style="width:60%" />
</p>

## Exponential Linear Unit (ELU) function, element-wise.

[FAST AND ACCURATE DEEP NETWORK LEARNING BY
EXPONENTIAL LINEAR UNITS (ELUS)](./assets/0701ELU.pdf)

### definition

$$
ELU(x) = \begin{cases}
   x &\text{if } x\gt0 \\
   \alpha * (exp(x)-1) &\text{if } x \leq 0
\end{cases}
$$

$$
ELU'(x) = \begin{cases}
1 &\text{if } x \gt0 \\
f(x) + \alpha &\text{if } x\leq 0
\end{cases}
$$

> ReLU í•¨ìˆ˜ë¥¼ ë¶€ë“œëŸ½ê²Œ ê¹ì€ í•¨ìˆ˜, $\alpha$ëŠ” ëŒ€ì²´ë¡œ 1

### ELU, LReLU, ReLU, SReLU ë¹„êµ
<p align="center">
<img src="./assets/0701ELU.png" style="width:35%" />
</p>

### íŠ¹ì§•
- ReLUì™€ LReLUê°€ ê·¸ë¬ë˜ ê²ƒì²˜ëŸ¼, ì–‘ìˆ˜ inputì„ ë°›ì•˜ì„ ë•Œ ê¸°ìš¸ê¸°ê°€ 1ì´ì–´ì„œ vanishing gradinetë¥¼ ì˜ˆë°©í•œë‹¤.
- ReLUê°€ ìŒìˆ˜ inputì— ëŒ€í•´ì„œ ë‹¨ìˆœíˆ 0ì˜ ê°’ì„ ì·¨í•˜ëŠ” ê²ƒì— ë°˜í•´ ELUëŠ” ìŒìˆ˜ì˜ ê°’ì„ ì·¨í•œë‹¤. ì´ëŠ” Mean activationsì„ 0ìœ¼ë¡œ (ì¼ì¢…ì˜ í¸í–¥ì´ë™ ê´€ì ) ë°€ì–´ì¤Œìœ¼ë¡œì¨ gradientê°€ natural gradient(ë¶„í¬ì˜ ê´€ì ì—ì„œì˜ gradient) ì— ë” ê°€ê¹Œì›Œ ì§€ê²Œ í•´ ë” ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
> - negative ì²´ì œ(regime)ì—ì„œ ë¶„ëª…í•œ saturation ê³ ì›(plateau)ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ë³´ë‹¤ ê°•ê±´í•œ í‘œí˜„(representations)ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ. íŠ¹íˆ 5ê°œ layer ì´ìƒì˜ íŠ¹ì •í•œ network êµ¬ì¡°ë¥¼ ê°€ì§„ ReLU ë° LReLUì— ë¹„í•´ ì¢€ ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ì¢€ ë” ë‚˜ì€ ì¼ë°˜í™”(generalization)ë¥¼ ì œê³µí•¨. ë˜í•œ, ReLUì˜ ë³€ì¢…ë“¤ì— ë¹„í•´ state-of-the-art ê²°ê³¼ë¥¼ ë³´ì¥í•¨.(??????) [ì¶œì²˜](https://neverabandon.tistory.com/8)

[Natural Gradientë¥¼ ìœ„í•´ ë³´ë©´ ì¢‹ì„ ê¸€](https://rlwithme.tistory.com/5)


## Hard Shrink function

### definition

$$
HardShrink(x) = \begin{cases}
   x &\text{if } x\gt\lambda \\
   x &\text{if } x\lt-\lambda \\
   0 &\text{otherwise}
\end{cases}
$$


<p align="center">
<img src="./assets/0703HardShrink.png" style="width:35%" />
</p>

## Soft Shrink function

### definition

$$
SoftShrinkage(x) = \begin{cases}
   x-\lambda &\text{if } x\gt\lambda \\
   x+\lambda &\text{if } x\lt-\lambda \\
   0 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703SoftShrink.png" style="width:35%" />
</p>


## Tanhshrink function

### definition

$$
Tanhshrink(x) = x - tanh(x)
$$

<p align="center">
<img src="./assets/0703TanhShrink.png" style="width:35%" />
</p>
---

> TanhShrinkì™€ HardShrinkëŠ” ì ì¬ë³€ìˆ˜ ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ í¬ì†Œì½”ë”©ì™¸ì—ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.  
[Shrink activation function Usage](https://deeesp.github.io/deep%20learning/DL-Activation-Functions/#hardshrink---nnhardshrink)
>> **ì ì¬ë³€ìˆ˜(Latent Variable)** : ì‹ í˜¸ë¥¼ ì´ë£¨ëŠ” ê¸°ì € ì‹ í˜¸ë“¤, ì§ì ‘ ì¸¡ì •í•  ìˆ˜ ì—†ì–´ í†µê³„ì  ë°©ë²•ë¡ ìœ¼ë¡œ ì¶”ì •í•¨.
>> **í¬ì†Œì½”ë”©** : ì ì¬ë³€ìˆ˜ë¥¼ ì•Œì•„ë‚´ê¸° ìœ„í•´ ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ëŠ” ê²ƒ(?) ê·¸ ê²°ê³¼ê°€ í¬ì†Œ í–‰ë ¬ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤(?)  
[í¬ì†Œì½”ë”©?](https://wordbe.tistory.com/134)
>> $$x=Da \\ \text{ where } D = (d_1d_2...d_m)$$
>> x : ì˜ìƒ(ì‹ í˜¸)  
>> $D$ : ì‚¬ì „(dictionary) (í¬ì†Œí–‰ë ¬)  
>> $a$ : í¬ì†Œì½”ë“œ (ê³„ìˆ˜ì§‘í•©)  
>> $d_i$ : ì‚¬ì „ìš”ì†Œ


## Hard Sigmoid fuction

### definition

$$
Hardsigmoid(x) = \begin{cases}
0 &\text{if } x\leq -3,\\
1 &\text{if } x \geq 3,\\
x/6 + 1/2 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardSigmoid.png" style="width:35%" />
</p>

[BinaryConnect Training Deep Neural Networks with binary weights during propagations](./assets/0703BinaryConnect_Training%20Deep%20Neural%20Networks%20with.pdf) ì—ì„œ ì²˜ìŒ ë„ì…ë¨

- binary weights(-1, 1) ë§Œìœ¼ë¡œ ë¹ ë¥´ê³  ì €ë ´í•˜ê²Œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“¤ê³  ì‹¶ì„ ë–„ ì‚¬ìš©í•˜ë©´ ì•„ì£¼ ì¢‹ë‹¤.
- software ì ìœ¼ë¡œë„ ê·¸ë ‡ê³ , ì–´ë–¤ hardware ê°€ì†ê¸°ë¥¼ ì‚¬ìš©í•˜ë“ ì§€ ìƒê´€ ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ê³  í•œë‹¤.


## Hard Tanh function

### definition

$$
HardTanh(x) = \begin{cases}
max\_val &\text{if } x\gt max\_val,\\
min\_val &\text{if } x \lt min\_val,\\
x &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardTanh.png" style="width:35%" />
</p>

> Tanh functionì˜ ê³„ì‚°ì ì¸ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ë²„ì „ì„.   
> ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì ìš© ë˜ì—ˆìŒ [ì¶œì²˜](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)

## Hard Swish(SiLU) function

### definition

$$
Hardswish(x) = \begin{cases}
0 &\text{if } x\leq -3,\\
x &\text{if } x \geq 3,\\
x \cdot (x+3)/6 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardSwish.png" style="width:35%" />
</p>

> ì‹ ê²½ë§ì—ì„œ ë” ê¹Šê²Œ ì§„í–‰í•¨ì— ë”°ë¼ nonlinearity ë¥¼ ì ìš©í•˜ëŠ” ë¹„ìš©ì€ ì ì  ì¤„ì–´ë“ ë‹¤. í•´ìƒë„(í”½ì…€ìˆ˜)ê°€ ë‹¤ìŒ ì¸µìœ¼ë¡œ ë„˜ì–´ê°ˆ ë–„ë§ˆë‹¤ ëŒ€ë¶€ë¶„ ì ˆë°˜ìœ¼ë¡œ ë–¨ì–´ì§€ê¸° ë–„ë¬¸ì´ë‹¤. `swish` ëª¨ë¸ ì—­ì‹œ ë§ì˜ ê¹Šì€ ë¶€ë¶„ì—ì„œ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ ëª¨ë¸ì—ì„œ `hard-swish` ë„ ëª¨ë¸ì˜ í›„ë°˜ë¶€ì—ì„œë§Œ ì‚¬ìš©í–ˆë‹¤. [ì¶œì²˜](0703SearchingforMobileNetV3)