# Non-linear Activations (weighted sum, nonlinearity)

1. Tensor  
2. Dataset,DataLoader  
   1. `torch.utils.data`
3. Transform
   1. `torchvision.transforms.v2`
      1. `v2.CutMix`
      2. `v2.MixUp`
   2. `torchvision.transforms`
4. nn-model  
   1. `torch.nn`
        1. Convolution Layers
            1. Convolution
               1. `nn.Conv#d`
               2. `nn.ConvTranspose#d`
        2. Pooling Layers
        3. **Non-Linear Activations**
5. Automatic-Differentiation  
6. Parameter-Optimization  
7. model-save-load  
---

## nn.Non-Linear Activations (weighted sum, nonlinearity)
- `nn.ELU` : Applies the Exponential Linear Unit (ELU) function, element-wise. *
- `nn.Hardshrink` : Applies the Hard Shrinkage (Hardshrink) function element-wise. *
- `nn.Hardsigmoid` : Applies the Hardsigmoid function element-wise. *
- `nn.Hardtanh` : Applies the HardTanh function element-wise. *
- `nn.Hardswish` : Applies the Hardswish function, element-wise. *
- `nn.LeakyReLU` : Applies the LeakyReLU function element-wise. *
- `nn.LogSigmoid` : Applies the Logsigmoid function element-wise. *
- `nn.MultiheadAttention` : Allows the model to jointly attend to information from different representation subspaces.
- `nn.PReLU` : Applies the element-wise PReLU function. *
- `nn.ReLU` : Applies the rectified linear unit function element-wise. *
- `nn.ReLU6` : Applies the ReLU6 function element-wise. *
- `nn.RReLU` : Applies the randomized leaky rectified linear unit function, element-wise. *
- `nn.SELU` : Applies the SELU function element-wise.
- `nn.CELU` : Applies the CELU function element-wise.
- `nn.GELU` : Applies the Gaussian Error Linear Units function.
- `nn.Sigmoid` : Applies the Sigmoid function element-wise.
- `nn.SiLU` : Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
- `nn.Mish` : Applies the Mish function, element-wise.
- `nn.Softplus` : Applies the Softplus function element-wise.
- `nn.Softshrink` : Applies the soft shrinkage function element-wise. *
- `nn.Softsign` : Applies the element-wise Softsign function.
- `nn.Tanh` : Applies the Hyperbolic Tangent (Tanh) function element-wise. *
- `nn.Tanhshrink` : Applies the element-wise Tanhshrink function. *
- `nn.Threshold` : Thresholds each element of the input Tensor.
- `nn.GLU` : Applies the gated linear unit function.

## Non-linear Activations (other)

- `nn.Softmin` : Applies the Softmin function to an n-dimensional input Tensor.
- `nn.Softmax` : Applies the Softmax function to an n-dimensional input Tensor.
- `nn.Softmax2d` : Applies SoftMax over features to each spatial location.
- `nn.LogSoftmax` : Applies the log(Softmax(ğ‘¥)) function to an n-dimensional input Tensor.
- `nn.AdaptiveLogSoftmaxWithLoss` : Efficient softmax approximation.

[Survey of Activation Functions](https://neverabandon.tistory.com/8)

[wikidocs.net](https://wikidocs.net/60683)

[ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì˜ ì˜ë¯¸ì™€ í•´ê²°ë°©ë²•](https://heytech.tistory.com/388)

[Gradient of ReLu at 0](https://discuss.pytorch.org/t/gradient-of-relu-at-0/64345/4)  

[Gradients for non-differentiable functions](https://pytorch.org/docs/stable/notes/autograd.html#gradients-for-non-differentiable-functions)

[Natural Gradientë¥¼ ìœ„í•´ ë³´ë©´ ì¢‹ì„ ê¸€](https://rlwithme.tistory.com/5)

## ì™œ Non-linear Activations ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

Linear Activationì˜ ê²½ìš° ë‹¨ìˆœí•œ í–‰ë ¬ ê³±ì¸ë°, ì´ëŠ” ë‹¨ìˆœíˆ ê´„í˜¸ë¥¼ í’€ì–´ ê³„ì‚°í•  ê²½ìš°ì— ë‹¨ í•œ ê°œì˜ Activation functionìœ¼ë¡œ ëŒ€ì²´ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì´ëŠ” ë§ì€ ì¸µì„ ì¶”ê°€í•´ê°€ë©´ì„œ ê° ì¸µë³„ë¡œ í™œì„±í•¨ìˆ˜ê°€ ì ìš©ë˜ì–´ nodeë“¤ ê°„ì˜ ì´˜ì´˜í•œ ê´€ê³„ë¥¼ êµ¬í˜„í•˜ê² ë‹¤ëŠ” ì‹ ê²½ë§ êµ¬í˜„ ë°©ì‹ê³¼ ë§ì§€ ì•Šë‹¤.

> ë‹¤ë§Œ, ì„ í˜• í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì¸µì„ 'ì€ë‹‰ì¸µ'(ë¹„ì„ í˜• í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”) ê³¼ êµ¬ë³„í•´ 'ì„ í˜•ì¸µ'ì´ë¼ëŠ” ë³„ê°œì˜ ì´ë¦„ìœ¼ë¡œ ë¶€ë¥´ê¸°ë„ í•  ì •ë„ë¡œ ê·¸ ìì²´ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” weightì´ ìƒê²¨ë‚œë‹¤ëŠ” ì¸¡ë©´ì—ì„œ ì˜ë¯¸ê°€ ì•„ì£¼ ì—†ì§€ëŠ” ì•Šë‹¤. [wikidocs.net](https://wikidocs.net/60683)

## í™œì„±í™” í•¨ìˆ˜ì˜ ì„ íƒ
[ì¶œì²˜](https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/)
- ì¼ë°˜ì ìœ¼ë¡œ SELU > ELU > LeakyReLU(ê·¸ë¦¬ê³  ë³€ì¢…ë“¤) > ReLU > tanh > sigmoid ìˆœ
- ë„¤íŠ¸ì›Œí¬ê°€ ìê¸° ì •ê·œí™”ë˜ì§€ ëª»í•˜ëŠ” êµ¬ì¡°ë¼ë©´ SELU ë³´ë‹¨ ELU
- ì‹¤í–‰ ì†ë„ê°€ ì¤‘ìš”í•˜ë‹¤ë©´ LeakyReLU(í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë” ì¶”ê°€í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´ ì¼€ë¼ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ê°’ Î± ì‚¬ìš©)
- ì‹œê°„ê³¼ ì»´í“¨íŒ… íŒŒì›Œê°€ ì¶©ë¶„í•˜ë‹¤ë©´ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•´ ì—¬ëŸ¬ í™œì„±í™” í•¨ìˆ˜ë¥¼ í‰ê°€
- ì‹ ê²½ë§ì´ ê³¼ëŒ€ì í•©ë˜ì—ˆë‹¤ë©´ RReLU
- í›ˆë ¨ì„¸íŠ¸ê°€ ì•„ì£¼ í¬ë‹¤ë©´ PReLU
- ReLUê°€ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì´ë¯€ë¡œ ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë“¤ì´ ReLUì— íŠ¹í™”ë˜ì–´ ìµœì í™”. ë”°ë¼ì„œ ì†ë„ê°€ ì¤‘ìš”í•˜ë‹¤ë©´ ReLUê°€ ê°€ì¥ ì¢‹ì€ ì„ íƒ

## Vanishing Gradient

[ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì˜ ì˜ë¯¸ì™€ í•´ê²°ë°©ë²•](https://heytech.tistory.com/388)

ê¸°ìš¸ê¸° ì†Œì‹¤ì´ë€ ì—­ì „íŒŒë¥¼ ì´ìš©í•œ weight ì¡°ì • ê³¼ì •ì—ì„œ Chain Ruleì— ë”°ë¼ ê³„ì†í•´ì„œ gradientë¥¼ ê³±í•˜ê²Œ ë˜ëŠ”ë°, ì´ gradient ê°’ì´ ê³„ì†í•´ì„œ ì‘ì•„ì ¸ ì…ë ¥ì¸µì— ê°€ê¹Œì›Œì¡Œì„ ë•Œ ì¯¤ì—” 0ì— ê·¼ì‚¬í•´ì§€ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

### Sigmoid í•¨ìˆ˜

$$
S(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}
$$

<p align="center">
<img src="./assets/0702sigmoid.png" style="width:35%" />
</p>


$$
S'(x) = S(x)(1-S(x))
$$

ë¯¸ë¶„ ê³„ì‚°ì˜ í¸ì˜ì„±ê³¼ ë‰´ëŸ°ì˜ ì‹ ê²½ì‹ í˜¸ ì „ë‹¬ ë°©ì‹ì„ ë§¤ìš° ê°€ê¹ê²Œ êµ¬í˜„í•œë‹¤ëŠ” ì ì—ì„œ ë„ë¦¬ ì‚¬ìš©ëë˜ Sigmoid í•¨ìˆ˜.
ê·¸ ë¯¸ë¶„ì€ $S(x) = 0.5$ ì—ì„œ ìµœëŒ“ê°’ 0.25ë¥¼ ê°€ì§„ë‹¤. ë”°ë¼ì„œ, ëª¨ë“  $x$ì˜ ë²”ìœ„ ë‚´ì—ì„œ ìµœëŒ€ 0.25 , ìµœì†Œ 0ì˜ ê°’ì„ ê°€ì§€ë¯€ë¡œ, ì´ ë¯¸ë¶„ê°’(gradient)ë¥¼ ê³„ì†í•´ì„œ ê³±í•˜ë‹¤ ë³´ë©´, ê·¸ gradientëŠ” ê·¹íˆ ì‘ì•„ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤.

> ì´ì— ë”í•´, exp(x)ì˜ ê³„ì‚°ì„ ê·¼ì‚¬ì¹˜ë¡œ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì»´í“¨í„°ì˜ ê³„ì‚°ì˜¤ì°¨ë„ ì—­ì‹œ ê³„ì†í•´ì„œ ê°€ì‚°ëœë‹¤.

#### Bias Shift(í¸í–¥ ì´ë™)
Sigmoid í•¨ìˆ˜ì˜ Vanishing gradientë¥¼ ê°€ì†ì‹œí‚¤ëŠ” ì´ìœ ê°€ í•˜ë‚˜ ë” ìˆëŠ”ë°, ë°”ë¡œ **Bias Shift** (í¸í–¥ ì´ë™) ì´ë‹¤.
Sigmoid í•¨ìˆ˜ì˜ í‰ê· ì€ 0ì´ ì•„ë‹Œ 0.25ì¸ë°, ì´ëŠ” ì¦‰ ì…ë ¥ê°’ì˜ í•©ë³´ë‹¤ ì¶œë ¥ì¸µì˜ í•©ì´ ë” ì»¤ì§ˆ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì´ê²ƒì€ ë‹¤ì‹œ, gradientì˜ ë¶„í¬ê°€ gradientì˜ ìµœëŒ“ê°’ì¸ 0.25ë¥¼ ê°€ì§€ëŠ” ì¤‘ê°„ë¶€ë³´ë‹¤ëŠ” ì–‘ ê·¹ë‹¨ì—ì„œ ê´€ì°°ë  í™•ë¥ ì´ ì ì  ì»¤ì§„ë‹¤ëŠ” ëœ»ì´ë‹¤.

### $\tanh$ í•¨ìˆ˜

$$
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

<p align="center">
<img src="./assets/0702tanh.png" style="width:35%" />
</p>

$$
f'(x) = 1-f(x)^2
$$

ìŒê³¡ íƒ„ì  íŠ¸ í•¨ìˆ˜ëŠ” ê·¸ í˜•íƒœê°€ sigmoidì™€ ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, ìµœëŒ“ê°’ 1ì„ ê°€ì ¸ sigmoidë³´ë‹¤ í›¨ì”¬ ë„‰ë„‰í•œ gradientë²”ìœ„ë¥¼ ê°€ì¡Œë‹¤. ê·¸ëŸ¬ë‚˜, ì—¬ì „íˆ ê³±í•  ìˆ˜ë¡ ì‘ì•„ì§€ëŠ” íŠ¹ì„±ì„ ë²—ì–´ë‚˜ì§€ëŠ” ëª»í–ˆë‹¤. ë‹¤ë§Œ, Bias Shift í˜„ìƒì€ í‰ê· ì´ 0ì´ê¸° ë•Œë¬¸ì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

> ìì—°ì–´ ì²˜ë¦¬, ìŒì„±ì¸ì‹ ì„ ìœ„í•œ **recurrent neural networksì—ì„œ ëŒ€ë¶€ë¶„ ì‚¬ìš©ë¨**

### ReLU (Rectified Linear Unit) í•¨ìˆ˜

$$
f(x) = max(0, x)
$$
<p align="center">
<img src="./assets/0702ReLU.png" style="width:35%" />
</p>

0ë³´ë‹¤ í° ë²”ìœ„ì—ì„œ ê¸°ìš¸ê¸° 1, ì‘ì€ ë²”ìœ„ì—ì„œ 0ì˜ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ë‹¤. 

> 0ì—ì„œëŠ” 0ì„ ëŒ€ì…í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ë“¯ í•˜ë‹¤.   
[Gradient of ReLu at 0](https://discuss.pytorch.org/t/gradient-of-relu-at-0/64345/4)  
[Gradients for non-differentiable functions](https://pytorch.org/docs/stable/notes/autograd.html#gradients-for-non-differentiable-functions)

ë”°ë¼ì„œ, vanishing gradient ë¬¸ì œëŠ” í•´ê²°ë˜ì—ˆì§€ë§Œ, í™œì„±í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ì§€ ëª»í•œ ë…¸ë“œëŠ” 0ì´ ê³±í•´ì ¸ ë‹¤ì‹œëŠ” í™œì„±í™” ë˜ì§€ ëª»í•œë‹¤ëŠ” (Dying ReLU) ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.

### Leaky ReLU í•¨ìˆ˜

$$
LeakyReLU(x) = max(0,x) + negative\_slope * min(0,x)
$$

<p align="center">
<img src="./assets/0702LeakyReLu.png" style="width:35%" />
</p>

ReLU í•¨ìˆ˜ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ìŒì˜ ì…ë ¥ì´ ë“¤ì–´ì™”ì„ ë–„, 0ì´ ì•„ë‹Œ ë§¤ìš° ì‘ì€ ê¸°ìš¸ê¸°ë¥¼ ê³±í•œë‹¤. ì´ë¡œì¨, ë‰´ëŸ°ì´ ì£½ëŠ” ê²ƒì„ ì˜ˆë°©í•œë‹¤.

---

[Survey of Activation Functions](https://neverabandon.tistory.com/8) : ê°ì¢… í™œì„±í•¨ìˆ˜ë“¤ì— ëŒ€í•´ ì˜ ì •ë¦¬í•´ ë†“ì€ ê¸€


<p align="center">
<img src="./assets/0702FamilyOfActivationFunctions.png" style="width:60%" />
</p>

## Exponential Linear Unit (ELU) function, element-wise.

[FAST AND ACCURATE DEEP NETWORK LEARNING BY
EXPONENTIAL LINEAR UNITS (ELUS)](./assets/0701ELU.pdf)

### definition

$$
ELU(x) = \begin{cases}
   x &\text{if } x\gt0 \\
   \alpha * (exp(x)-1) &\text{if } x \leq 0
\end{cases}
$$

$$
ELU'(x) = \begin{cases}
1 &\text{if } x \gt0 \\
f(x) + \alpha &\text{if } x\leq 0
\end{cases}
$$

> ReLU í•¨ìˆ˜ë¥¼ ë¶€ë“œëŸ½ê²Œ ê¹ì€ í•¨ìˆ˜, $\alpha$ëŠ” ëŒ€ì²´ë¡œ 1

### ELU, LReLU, ReLU, SReLU ë¹„êµ
<p align="center">
<img src="./assets/0701ELU.png" style="width:35%" />
</p>

### íŠ¹ì§•
- ReLUì™€ LReLUê°€ ê·¸ë¬ë˜ ê²ƒì²˜ëŸ¼, ì–‘ìˆ˜ inputì„ ë°›ì•˜ì„ ë•Œ ê¸°ìš¸ê¸°ê°€ 1ì´ì–´ì„œ vanishing gradinetë¥¼ ì˜ˆë°©í•œë‹¤.
- ReLUê°€ ìŒìˆ˜ inputì— ëŒ€í•´ì„œ ë‹¨ìˆœíˆ 0ì˜ ê°’ì„ ì·¨í•˜ëŠ” ê²ƒì— ë°˜í•´ ELUëŠ” ìŒìˆ˜ì˜ ê°’ì„ ì·¨í•œë‹¤. ì´ëŠ” Mean activationsì„ 0ìœ¼ë¡œ (ì¼ì¢…ì˜ í¸í–¥ì´ë™ ê´€ì ) ë°€ì–´ì¤Œìœ¼ë¡œì¨ gradientê°€ natural gradient(ë¶„í¬ì˜ ê´€ì ì—ì„œì˜ gradient) ì— ë” ê°€ê¹Œì›Œ ì§€ê²Œ í•´ ë” ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í•œë‹¤.
> - negative ì²´ì œ(regime)ì—ì„œ ë¶„ëª…í•œ saturation ê³ ì›(plateau)ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ë³´ë‹¤ ê°•ê±´í•œ í‘œí˜„(representations)ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ. íŠ¹íˆ 5ê°œ layer ì´ìƒì˜ íŠ¹ì •í•œ network êµ¬ì¡°ë¥¼ ê°€ì§„ ReLU ë° LReLUì— ë¹„í•´ ì¢€ ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ì¢€ ë” ë‚˜ì€ ì¼ë°˜í™”(generalization)ë¥¼ ì œê³µí•¨. ë˜í•œ, ReLUì˜ ë³€ì¢…ë“¤ì— ë¹„í•´ state-of-the-art ê²°ê³¼ë¥¼ ë³´ì¥í•¨.(??????) [ì¶œì²˜](https://neverabandon.tistory.com/8)

[Natural Gradientë¥¼ ìœ„í•´ ë³´ë©´ ì¢‹ì„ ê¸€](https://rlwithme.tistory.com/5)

## SELU (Scaled ELU)

### definition

$$
SELU(x) = scale*(max(0,x) + min(0,\alpha*(exp(x)-1)))
$$

with $\alpha = 1.6732632423543772848170429916717$ and  
$scale = 1.0507009873554804934193349852946$

$$
selu(x) = \lambda\begin{cases}
x &\text{if } x\gt 0 \\
\alpha e^x - \alpha &\text{if } x\leq 0
\end{cases}
$$


<p align="center">
<img src="./assets/0709SELU.png" style="width:35%" />
</p>

### íŠ¹ì§•
[Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515)

Self-Normalizing NN ì„ ë§Œë“¤ê¸° ìœ„í•´ ê³ ì•ˆëœ í™œì„±í•¨ìˆ˜ë¡œ, ê°ê°ì˜ í™œì„±í•¨ìˆ˜ $y$ì— ëŒ€í•´ ì´ì „ ì¸µì—ì„œì˜ í‰ê· ê³¼ ë¶„ì‚°ì— í•¨ìˆ˜ $g$ ë¥¼ ì ìš©í•´ ë‹¤ìŒ ì¸µì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•  ìˆ˜ ìˆë‹¤ë©´, ì´ NNì„ Self-normalizing í•˜ë‹¤ê³  ì •ì˜í–ˆë‹¤.

(scaled) ReLUë¡œëŠ” í•  ìˆ˜ ì—†ì–´ì„œ (scaled) ELU ë¥¼ ì‚¬ìš©í–ˆë‹¤ëŠ”ë°, ì›í•˜ëŠ” í™œì„±í•¨ìˆ˜ì˜ íŠ¹ì§•ì€,

1. í‰ê· ì„ ì¡°ì ˆí•˜ëŠ” ìŒê³¼ ì–‘ì˜ ê°’ë“¤
2. saturation regions (0ì— ê·¼ì ‘í•˜ëŠ” derivative) : ë‚®ì€ ì¸µì—ì„œ ë¶„ì‚°ì´ ë„ˆë¬´ í¬ë©´ ì´ë¥¼ ì™„í™”ì‹œì¼œì£¼ê¸° ìœ„í•´ì„œ
3. 1ë³´ë‹¤ í° ê¸°ìš¸ê¸° : ë‚®ì€ ì¸µì—ì„œ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ë¥¼ í‚¤ì›Œì£¼ê¸° ìœ„í•´ì„œ
4. ì—°ì†

ì´ë¼ê³  ì£¼ì¥

> ì™„ì „ ì—°ê²°ì¸µë§Œ ìŒ“ì•„ì„œ ì‹ ê²½ë§ì„ ë§Œë“¤ê³  ëª¨ë“  ì€ë‹‰ì¸µì„ SELU ë¥¼ ì“°ë©´ ì‹ ê²½ë§ì´ self-normalized ëœë‹¤ê³  ì£¼ì¥  
> í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì¶œë ¥ì´ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì„ ìœ ì§€  
> ì¢…ì¢… ë‹¤ë¥¸ í™œì„± í•¨ìˆ˜ë³´ë‹¤ ë›°ì–´ë‚˜ì§€ë§Œ, ì¡°ê±´ì´ ê¹Œë‹¤ë¡œì›€  
> 1. inputì´ í‘œì¤€í™” ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
> 2. ëª¨ë“  ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” ë¥´ì¿¤ ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”
> 3. ë„¤íŠ¸ì›Œí¬ëŠ” ì¼ë ¬ë¡œ ìŒ“ì—¬ì•¼ í•¨. RNN ê°™ì´ ìˆœì°¨ì ì´ì§€ ì•Šìœ¼ë©´ self-normalized ë³´ì¥ì´ ì•ˆë¨  
> [ì¶œì²˜](https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/)



## Hard Shrink function

### definition

$$
HardShrink(x) = \begin{cases}
   x &\text{if } x\gt\lambda \\
   x &\text{if } x\lt-\lambda \\
   0 &\text{otherwise}
\end{cases}
$$


<p align="center">
<img src="./assets/0703HardShrink.png" style="width:35%" />
</p>

## Soft Shrink function

### definition

$$
SoftShrinkage(x) = \begin{cases}
   x-\lambda &\text{if } x\gt\lambda \\
   x+\lambda &\text{if } x\lt-\lambda \\
   0 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703SoftShrink.png" style="width:35%" />
</p>


## Tanhshrink function

### definition

$$
Tanhshrink(x) = x - tanh(x)
$$

<p align="center">
<img src="./assets/0703TanhShrink.png" style="width:35%" />
</p>
---

> TanhShrinkì™€ HardShrinkëŠ” ì ì¬ë³€ìˆ˜ ê°’ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ í¬ì†Œì½”ë”©ì™¸ì—ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.  
[Shrink activation function Usage](https://deeesp.github.io/deep%20learning/DL-Activation-Functions/#hardshrink---nnhardshrink)
>> **ì ì¬ë³€ìˆ˜(Latent Variable)** : ì‹ í˜¸ë¥¼ ì´ë£¨ëŠ” ê¸°ì € ì‹ í˜¸ë“¤, ì§ì ‘ ì¸¡ì •í•  ìˆ˜ ì—†ì–´ í†µê³„ì  ë°©ë²•ë¡ ìœ¼ë¡œ ì¶”ì •í•¨.
>> **í¬ì†Œì½”ë”©** : ì ì¬ë³€ìˆ˜ë¥¼ ì•Œì•„ë‚´ê¸° ìœ„í•´ ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ëŠ” ê²ƒ(?) ê·¸ ê²°ê³¼ê°€ í¬ì†Œ í–‰ë ¬ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤(?)  
[í¬ì†Œì½”ë”©?](https://wordbe.tistory.com/134)
>> $$x=Da \\ \text{ where } D = (d_1d_2...d_m)$$
>> x : ì˜ìƒ(ì‹ í˜¸)  
>> $D$ : ì‚¬ì „(dictionary) (í¬ì†Œí–‰ë ¬)  
>> $a$ : í¬ì†Œì½”ë“œ (ê³„ìˆ˜ì§‘í•©)  
>> $d_i$ : ì‚¬ì „ìš”ì†Œ


## Hard Sigmoid fuction

### definition

$$
Hardsigmoid(x) = \begin{cases}
0 &\text{if } x\leq -3,\\
1 &\text{if } x \geq 3,\\
x/6 + 1/2 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardSigmoid.png" style="width:35%" />
</p>

[BinaryConnect Training Deep Neural Networks with binary weights during propagations](./assets/0703BinaryConnect_Training%20Deep%20Neural%20Networks%20with.pdf) ì—ì„œ ì²˜ìŒ ë„ì…ë¨

- binary weights(-1, 1) ë§Œìœ¼ë¡œ ë¹ ë¥´ê³  ì €ë ´í•˜ê²Œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“¤ê³  ì‹¶ì„ ë–„ ì‚¬ìš©í•˜ë©´ ì•„ì£¼ ì¢‹ë‹¤.
- software ì ìœ¼ë¡œë„ ê·¸ë ‡ê³ , ì–´ë–¤ hardware ê°€ì†ê¸°ë¥¼ ì‚¬ìš©í•˜ë“ ì§€ ìƒê´€ ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ê³  í•œë‹¤.


## Hard Tanh function

### definition

$$
HardTanh(x) = \begin{cases}
max\_val &\text{if } x\gt max\_val,\\
min\_val &\text{if } x \lt min\_val,\\
x &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardTanh.png" style="width:35%" />
</p>

> Tanh functionì˜ ê³„ì‚°ì ì¸ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ë²„ì „ì„.   
> ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì ìš© ë˜ì—ˆìŒ [ì¶œì²˜](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)

## Hard Swish(SiLU) function

### definition

$$
Hardswish(x) = \begin{cases}
0 &\text{if } x\leq -3,\\
x &\text{if } x \geq 3,\\
x \cdot (x+3)/6 &\text{otherwise}
\end{cases}
$$

<p align="center">
<img src="./assets/0703HardSwish.png" style="width:35%" />
</p>

> ì‹ ê²½ë§ì—ì„œ ë” ê¹Šê²Œ ì§„í–‰í•¨ì— ë”°ë¼ nonlinearity ë¥¼ ì ìš©í•˜ëŠ” ë¹„ìš©ì€ ì ì  ì¤„ì–´ë“ ë‹¤. í•´ìƒë„(í”½ì…€ìˆ˜)ê°€ ë‹¤ìŒ ì¸µìœ¼ë¡œ ë„˜ì–´ê°ˆ ë–„ë§ˆë‹¤ ëŒ€ë¶€ë¶„ ì ˆë°˜ìœ¼ë¡œ ë–¨ì–´ì§€ê¸° ë–„ë¬¸ì´ë‹¤. `swish` ëª¨ë¸ ì—­ì‹œ ë§ì˜ ê¹Šì€ ë¶€ë¶„ì—ì„œ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ì•Œê³  ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ ëª¨ë¸ì—ì„œ `hard-swish` ë„ ëª¨ë¸ì˜ í›„ë°˜ë¶€ì—ì„œë§Œ ì‚¬ìš©í–ˆë‹¤. [ì¶œì²˜](0703SearchingforMobileNetV3)

## Rectified Linear Unit(ReLU) Function

### definition

$$
f(x) = max(0, x)
$$
<p align="center">
<img src="./assets/0702ReLU.png" style="width:35%" />
</p>

0ë³´ë‹¤ í° ë²”ìœ„ì—ì„œ ê¸°ìš¸ê¸° 1, ì‘ì€ ë²”ìœ„ì—ì„œ 0ì˜ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§„ë‹¤. 

### ì¥ì 
- ë¹ ë¥¸ í•™ìŠµ
- Sigmoid, Tanh actiation functionì— ë¹„í•´ì„œ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë° ì¼ë°˜í™”
- linear model ì˜ ì¥ì ì¸ gradient-descent ë¥¼ ìµœì í™”ì— ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì„ ê³„ìŠ¹í•¨
- ë¹ ë¥¸ ê³„ì‚° (ì§€ìˆ˜, ë‚˜ëˆ—ì…ˆ ì—†ìŒ)
> 0ì—ì„œ ìµœëŒ€ ì‚¬ì´ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ëŠ” ê°’ë“¤ì„ ë­‰ê°œë²„ë¦¼(squishes)ìœ¼ë¡œì¨ hidden unitsì— í¬ì†Œì„±(sparcity)ì„ ë„ì…í•  ìˆ˜ ìˆìŒ (?)

### ë‹¨ì 
- Sigmoid ì— ë¹„í•´ ì‰½ê²Œ Overfittingë˜ëŠ” ê²½í–¥ì´ ìˆê³ , ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ dropout ê¸°ë²•ì´ ì ìš©ë¨.
- dead neuronì´ í•™ìŠµì— ë°©í•´ë¥¼ ì•¼ê¸°í•¨

## Leaky ReLu (LReLU)

### definition

$$
LeakyReLU(x) = max(0,x) + negative\_slope * min(0,x)
$$

<p align="center">
<img src="./assets/0702LeakyReLu.png" style="width:35%" />
</p>

- ì£½ì€ ë‰´ëŸ°ì´ ì•ˆ ìƒê¸´ë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” ReLUì™€ ë™ì¼í•¨
- í¬ì†Œì„±ê³¼ ë¶„ì‚°ì„ ê°€ì§„ë‹¤ëŠ” ì ì„ ë¹¼ë©´ ìƒë‹¹í•œ ê°œì„ ì€ ì—†ìŒ

## LogSigmoid

### definition

$$
LogSigmoid(x) = log(\frac{1}{1+exp(-x)})
$$

<p align="center">
<img src="./assets/0704LogSigmoid.png" style="width:35%" />
</p>

[log sigmoid ê·¼ì‚¬](https://bab2min.tistory.com/626)

ìœ„ ë§í¬ì—ì„œëŠ”,
- word2vecì„ í™•ì¥í•œ ëª¨í˜•ì—ì„œ LogSigmoidë¥¼ ì‚¬ìš©í•œ ì½”ë“œë¥¼ êµ¬í˜„í–ˆëŠ”ë°
- ê·¸ ê³„ì‚° ë¹„ìš©ì— ì§€ìˆ˜ì™€ ë¡œê·¸ê°€ í¬í•¨ë˜ì–´ ì „ì²´ì ì¸ ì„±ëŠ¥í–¥ìƒì´ í•„ìš”í–ˆë‹¤.
- ê·¼ì‚¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í–ˆëŠ”ë°, êµ¬ì²´ì ì¸ ë°©ë²•ì€ ì´ë ‡ë‹¤.
- x<0 ì—ì„œ
   1. S(x) = logsigmoid(x)
   2. S(x) ~ $-log(e^(-x))$ = x
   3. S(x) - x = ì˜¤ì°¨ = $log(1+e^x)$ = S(-x)
- ì¸ë° x>0 ì—ì„œ í•¨ìˆ˜ëŠ” 0ì— ë§¤ìš° ë¹¨ë¦¬ ê·¼ì ‘í•˜ë¯€ë¡œ, 0ì— ì•„ì§ ì¶©ë¶„íˆ ê°€ê¹ì§€ ì•Šì€ ë¶€ë¶„ê¹Œì§€ë§Œ(ëŒ€ëµ x=32) í…Œì´ë¸”ë¡œ êµ¬í•´ë†“ê³ , ê·¸ ì´ìƒì€ 0ìœ¼ë¡œ í‰ì¹˜ëŠ” ë°©ë²•ì„ ì¼ë‹¤.
> ì†ì‹¤í•¨ìˆ˜ë¡œ ë” ë§ì´ ì“°ì¸ë‹¤ëŠ” ê²ƒì„ ì œì™¸í•˜ê³ ëŠ” ë‚˜ì˜¤ëŠ” ê²Œ ë³„ë¡œ ì—†ë‹¤.

## Multi-Head Attention

Transformer ë¬¸ì„œì— ë³„ë„ ì‘ì„±

## PReLU

### definition

$$
PReLU(x) = max(0,x) + a*min(0,x) \\ or
\\  
PReLU(x) = \begin{cases}
x &\text{if } x\geq0\\
ax &\text{otherwise}
\end{cases}
$$

$a$ëŠ” í•™ìŠµ íŒŒë¼ë¯¸í„°ë¡œ, `num_parameters=1`ì— ì•„ë¬´ê²ƒë„ ë„˜ê²¨ ì£¼ì§€ ì•Šì„ ê²½ìš° (ì¦‰, default ì¸ 1ì˜ ê°’ì„ ì¤„ ê²½ìš°) aëŠ” ë‹¨ì¼ íŒŒë¼ë¯¸í„°ë¡œ, ëª¨ë“  input channelë“¤ì— ë™ì¼í•˜ê²Œ ì ìš©ëœë‹¤.
ë§Œì•½ `nn.PReLU(nChannels)`ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì •í•´ì¤€ë‹¤ë©´, ê° input channelë§ˆë‹¤ ë‹¤ë¥¸ $a$ê°€ ì ìš©ëœë‹¤.
(nChannelsëŠ” inputì˜ 2nd ì°¨ì›ì„ ê°€ë¦¬í‚¤ëŠ”ë°, inputì˜ ì°¨ì›ì´ 2 ë¯¸ë§Œì´ë¼ë©´, 1ë¡œ ê°„ì£¼í•œë‹¤.)

`init=0.25`ì— $a$ì˜ ì´ˆê¸°ê°’ì„ ì ìš©í•´ ì¤„ ìˆ˜ ìˆë‹¤.

> weight decayëŠ” ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì•„ì•¼ í•œë‹¤.

### íŠ¹ì§•
$a$ëŠ” ìŒìˆ˜ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ë³€ìˆ˜ì¸ë°, back-prop ê³¼ì •ì—ì„œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤. $a=0$ì¸ ê²½ìš° PReLUëŠ” ReLUì™€ ê°™ì•„ì§„ë‹¤.

> Large scale Image Recognitionì—ì„œ ReLUë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤. Visual Recognition Challengeì—ì„œ ì‚¬ëŒ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ì²˜ìŒìœ¼ë¡œ ë„˜ì—ˆë‹¤ê³  í•œë‹¤. [ì¶œì²˜](https://neverabandon.tistory.com/8)

### Optimization

<p align="center">
<img src="./assets/0706PReLUOptimization.png" style="width:60%" />
</p>

ì¶œì²˜:
[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)

ìµœì í™” ê³¼ì •ì´ ê¶ê¸ˆí•´ ì°¾ì•„ë³´ì•˜ë‹¤. ì‹¤ì œë¡œëŠ” momentum(4ë²ˆ ì‹ì—ì„œ $\mu$)ì„ ì ìš©í•˜ë©°, ìˆ˜ì‹ì€ ìœ„ì— ì¨ì ¸ ìˆëŠ” ê²ƒê³¼ ê°™ë‹¤. ê·¸ ê³¼ì •ì—ì„œ weight decay (= l2 ì •ê·œí™”) ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ $a$ì˜ ì ˆëŒ“ê°’ì´ ê³„ì†í•´ì„œ ì¤„ì–´ë“¤ë©´ì„œ 0ì´ ë˜ì–´ ReLUì™€ ê°™ì•„ì§€ê¸° ë•Œë¬¸ì—, ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•œë‹¤. ì •ê·œí™” ì—†ì´ë„ ìµœëŒ“ê°’ì´ 1ì„ ë„˜ì§€ ì•Šì•˜ê³ , í™œì„±í•¨ìˆ˜ê°€ ë‹¨ì¡° ì¦ê°€í•˜ì§€ ì•Šë„ë¡ $a$ì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ì§€ë„ ì•Šì•˜ë‹¤ê³  í•œë‹¤. ëª©ì í•¨ìˆ˜ì— ëŒ€í•œ $a$ì˜ gradientë¥¼ ê³„ì‚°í•  ë•Œ layerì˜ ëª¨ë“  ì±„ë„ì— ëŒ€í•´ ë”í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ì´ ì •ë„ì˜ ì‹œê°„ë³µì¡ë„ëŠ” forward, backprop ëª¨ë‘ì—ì„œ ë¬´ì‹œê°€ëŠ¥í•œ ìˆ˜ì¤€ì´ë¼ê³  í•œë‹¤.


## ReLU6

[AlexNet - Alex Krizhevsky](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf)

[Jinsol Kim's Blog](https://gaussian37.github.io/dl-concept-relu6/)

### definition

$$
ReLU6(x) = min(max(0,x), 6)
$$

<p align="center">
<img src="./assets/0707ReLU6.png" style="width:35%" />
</p>

**ìƒí•œì„ ì„ 6ìœ¼ë¡œ ë‘” ReLU** í•¨ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì—¬ê¸°ì„œ 

**ìƒí•œì„ ì„ ë‘ëŠ” ì´ìœ **ëŠ”

- (embedded ì˜ì—­ì—ì„œ íŠ¹íˆ) í‘œí˜„í•˜ëŠ”ë° í•„ìš”í•œ bitë¥¼ ì ˆì•½í•˜ê¸° ìœ„í•´ì„œ,
- sparseí•œ featureë¥¼ ë” ì¼ì° í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ê¸° ë•Œë¬¸ì—, ì´ë©°

**ê·¸ ìˆ«ìê°€ 6ì¸ ì´ìœ **ëŠ”  

- ë‹¨ìˆœíˆ ì„±ëŠ¥ì´ ì¢‹ì•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤.

### ë¶„í¬ ì¸¡ë©´
biasë§Œí¼ í‰í–‰ì´ë™í•œ 6ê°œì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¡œ êµ¬ì„±ëœ ReLU ìœ ë‹›

 vs ë¬´í•œê°œì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ ë¡œ ì¼ë°˜ì ì¸ ReLUì™€ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í–ˆë‹¤.

ì´ì— ë”°ë¼ Noise Model ì˜ í‘œì¤€í¸ì°¨ë„ ë³€í˜•ëœ ê²ƒì„ ì‚¬ìš©í–ˆëŠ”ë°,
ì›ë˜ëŠ” $\frac{1}{1+e^{-x}}$ ì˜ í‘œì¤€í¸ì°¨ë¥¼ ì§€ë‹Œ ì •ê·œë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ Alex KrizhevskyëŠ”

$$
\begin{cases}
0 &\text{if } y= 0 \text{ or } y=6 \\
1 &\text{if } 0 \lt y \lt 6
\end{cases}
$$

ë¥¼ ì‚¬ìš©í–ˆë‹¤.

ì´ë¥¼ ìš”ì•½í•˜ë©´, $y$ ê°€ 6ì— ê·¼ì ‘í–ˆì„ ë•ŒëŠ” ë‘ ëª¨ë¸ì´ ê±°ì˜ ê°™ì•„ì§€ì§€ë§Œ, 0ë¶€ê·¼ì—ì„œëŠ” 0ì„ ì¡°ê¸ˆì´ë¼ë„ ì´ˆê³¼í•˜ë©´ ê°‘ìê¸° ë§¤ìš° í° í¸ì°¨ì˜ noise í˜ë„í‹°ë¥¼ ì£¼ëŠ” ê²ƒì´ë‹¤. 

ì´ëŠ” ìì—°ì´ë¯¸ì§€ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµì´ ë„ë¦¬ ì±„ìš©í•˜ëŠ” sparseness-inducing tricksì— ê¸°ì¸í•œ ê²ƒì´ë‹¤.

Alex Krizhevskyì˜ ëª¨ë¸ì˜ filterë“¤ì€ weights ëŠ” ê³µìœ í•˜ì§€ë§Œ, biasëŠ” ê³µìœ í•˜ì§€ ì•ŠëŠ”ë°, ì´ëŠ” ë˜í•œ "ì´ë¯¸ì§€ì˜ êµ³ì´ ëª¨ë“  ë¶€ë¶„ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ”" í•„í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.(~~ì§ì—­í•˜ë‹ˆ ì–´ë ¤ìš´ë°, ê·¸ëƒ¥ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•´ í•™ìŠµí•œë‹¤ëŠ” ëœ»ì¸ ê²ƒ ê°™ë‹¤.~~)

> CNN ì„ í™•ë¥  ë¶„í¬ ì ì¸ ì¸¡ë©´ìœ¼ë¡œ ë°”ë¼ë³´ëŠ” ê²ƒì´ë¼ êµ‰ì¥íˆ ë‚¯ì„¤ê³  ëª¨ë¥´ëŠ” ê°œë…ì´ ë§ì´ ë‚˜ì˜¤ëŠ”ë°, (mean-field $y$,, energy,, sparse feature) ì¢€ ë” ê³µë¶€í•´ ë´ì•¼ ê² ë‹¤. Alex Krizhevskyê°€ ìê¸° ëª¨ë¸ì˜ ì›ë¥˜ë¡œì„œ ì°¸ê³ í•œ ë…¼ë¬¸ì€ [ReLU improve Restricted Boltzmann Machines](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)ì´ë‹¤.


## RReLU (randomized leaky ReLU)

### definition

$$
RReLU(x) = \begin{cases}
x &\text{if } x\geq 0 \\
ax &\text{otherwise}
\end{cases}
$$


<p align="center">
<img src="./assets/0708RReLU.png" style="width:35%" />
</p>

training ì¤‘ì—ëŠ” $a$ëŠ” uniform ë¶„í¬ $\mu(lower,upper)$ì—ì„œ ëœë¤ ìƒ˜í”Œë§ ëœ ê°’ì´ë‹¤. ë°˜ë©´ evaluation/test ê³¼ì •ì—ì„œ $a$ëŠ” ì¤‘ê°„ê°’($\frac{lower+upper}{2}$)ìœ¼ë¡œ ê³ ì •ëœë‹¤.

Kaggle NDSB ëŒ€íšŒì—ì„œ ì²˜ìŒ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, í•´ë‹¹ ëŒ€íšŒ ìš°ìŠ¹ìëŠ” $\mu(3,8)$ ì„ ì‚¬ìš©í–ˆë‹¤.

[Empirical Evaluation of Rectified Activations in Convolution Network](https://arxiv.org/pdf/1505.00853) ì— ë”°ë¥´ë©´, ReLU ë³´ë‹¤ Leaky ReLU, PReLU(Parametric ReLU), RReLUê°€ ë” ì„±ëŠ¥ì´ ì¢‹ì•˜ìœ¼ë‚˜, ê·¸ ì´ìœ ì— ëŒ€í•´ì„œëŠ” ì•„ì§ ë…¼ì˜ê°€ ë” í•„ìš”í•˜ë‹¤ê³  ë°í˜”ë‹¤. íŠ¹íˆ ë°ì´í„°ì…‹ì˜ í¬ê¸°ì— ë”°ë¼(ê·¸ì¤‘ì—ì„œë„, ë” ê±°ëŒ€í•œ ë°ì´í„°ì…‹ ì—ì„œ.) í™œì„±í•¨ìˆ˜ë“¤ì´ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€ì— ëŒ€í•´ ë” ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤ê³  ê²°ë¡ ì§€ì—ˆë‹¤.

