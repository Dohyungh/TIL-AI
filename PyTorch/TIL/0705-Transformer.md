# Transformer

1. Tensor  
2. Dataset,DataLoader  
   1. `torch.utils.data`
3. Transform
   1. `torchvision.transforms.v2`
      1. `v2.CutMix`
      2. `v2.MixUp`
   2. `torchvision.transforms`
4. nn-model  
   1. `torch.nn`
        1. Convolution Layers
            1. Convolution
               1. `nn.Conv#d`
               2. `nn.ConvTranspose#d`
        2. Pooling Layers
        3. Non-Linear Activations
            1. **Transformer**
                1. `nn.MultiheadAttention`
5. Automatic-Differentiation  
6. Parameter-Optimization  
7. model-save-load  
---

[pytorch로 구현하는 Transformer(Attention is All You Need)](https://cpm0722.github.io/pytorch-implementation/transformer)

[pytorch 공식 구현체로 보는 transformer MultiheadAttention과 numpy로 구현하기](https://hi-lu.tistory.com/entry/torch-%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EB%A1%9C-%EB%B3%B4%EB%8A%94-transformer-MultiheadAttention%EA%B3%BC-numpy%EB%A1%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0)

[Attention Layer](https://tkayyoo.tistory.com/135)

[Attention Is All You Need](./assets/0705AttentionIsAllYouNeed.pdf)

[`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)

[Transformer의 Multi-head Attention 파헤치기](https://seungseop.tistory.com/29)

[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)


[Transformer: A Novel Nerual Network Architecture for Language Understanding](https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/)

[0705ImageTransformer.pdf](./assets/0705ImageTransformer.pdf)

[0705LayerNormalization.pdf](./assets/0705LayerNormalization.pdf)

[0705MusicTransformer.pdf](./assets/0705MusicTransformer.pdf)
